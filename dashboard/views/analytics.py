# dashboard/views/analytics.py

import streamlit as st
import pandas as pd
from datetime import date, timedelta

from data_access import get_connection


def load_df(query: str, params=None) -> pd.DataFrame:
    conn = get_connection()
    # âŒ ne pas fermer la connexion ici, elle est gÃ©rÃ©e par data_access / Streamlit
    df = pd.read_sql(query, conn, params=params)
    return df


def render():
    st.title("ğŸ“Š Analytics Insights")

    st.markdown(
        """
        Cette section prÃ©sente les analyses avancÃ©es calculÃ©es Ã  partir des donnÃ©es TV / presse :  

        - **Media Bias Scores** : rÃ©partition des thÃ¨mes par mÃ©dia  
        - **Topic Spikes** : emballements mÃ©diatiques (pics de couverture)  
        - **Keyword Lifetime** : durÃ©e de vie des mots-clÃ©s dans les mÃ©dias  
        - **Topic Lifetime** : apparition, pic et disparition des sujets  
        - **Theme Lifetime** : persistance des grands thÃ¨mes narratifs  
        """
    )

    analysis = st.selectbox(
        "Choisissez une analyse Ã  explorer :",
        [
            "Media Bias",
            "Topic Spikes",
            "Keyword Lifetime",
            "Topic Lifetime",
            "Theme Lifetime",
        ],
    )

    if analysis == "Media Bias":
        st.subheader("ğŸ“Œ Media Bias Scores (par thÃ¨me et source)")

        query = """
            SELECT date, source, theme, bias_score, methodology
            FROM media_bias_scores
            ORDER BY date DESC, source, theme
            LIMIT 500
        """
        df = load_df(query)

        if df.empty:
            st.info("Aucun score de biais disponible pour lâ€™instant.")
            return

        # ğŸ”§ Normaliser la colonne date au format date (pas Timestamp)
        df["date"] = pd.to_datetime(df["date"]).dt.date

        # Filtre source et date
        sources = sorted(df["source"].unique())
        selected_sources = st.multiselect("Filtrer les sources :", sources, default=sources)

        min_date = df["date"].min()
        max_date = df["date"].max()
        date_range = st.slider(
            "PÃ©riode",
            min_value=min_date,
            max_value=max_date,
            value=(min_date, max_date),
        )

        # ğŸ”§ Ici on compare date Ã  date, sans pd.to_datetime
        mask = (
            df["source"].isin(selected_sources)
            & (df["date"] >= date_range[0])
            & (df["date"] <= date_range[1])
        )
        df_filtered = df[mask]

        st.dataframe(df_filtered)

        st.markdown("**Moyenne des biais par thÃ¨me et par mÃ©dia :**")
        bias_summary = (
            df_filtered.groupby(["source", "theme"])["bias_score"]
            .mean()
            .reset_index()
            .sort_values("bias_score", ascending=False)
        )
        st.dataframe(bias_summary)


    elif analysis == "Topic Spikes":
        st.subheader("ğŸ“Œ Topic Spikes Detection")

        query = """
            SELECT date, topic_id, source, spike_score, baseline_window
            FROM spikes
            ORDER BY date DESC, spike_score DESC
            LIMIT 500
        """
        df = load_df(query)

        if df.empty:
            st.info("Aucun spike dÃ©tectÃ© pour lâ€™instant (pas assez dâ€™historique).")
            return

        st.dataframe(df)

        top_spikes = df.sort_values("spike_score", ascending=False).head(20)
        st.markdown("**Top spikes (score le plus Ã©levÃ©) :**")
        st.dataframe(top_spikes)

    elif analysis == "Keyword Lifetime":
        st.subheader("â³ Keyword Lifetime")

        query = """
            SELECT word, start_date, end_date, duration_days, total_frequency
            FROM keyword_lifetime
            ORDER BY start_date DESC
            LIMIT 500
        """
        df = load_df(query)

        if df.empty:
            st.info("Aucune donnÃ©e de lifetime pour les mots-clÃ©s.")
            return

        st.dataframe(df)

        st.markdown("**Mots-clÃ©s les plus persistants (par durÃ©e) :**")
        longest = df.sort_values("duration_days", ascending=False).head(20)
        st.dataframe(longest)

        st.markdown("**Mots-clÃ©s les plus frÃ©quents (par mentions totales) :**")
        most_freq = df.sort_values("total_frequency", ascending=False).head(20)
        st.dataframe(most_freq)

    elif analysis == "Topic Lifetime":
        st.subheader("ğŸ“† Topic Lifetime")

        query = """
            SELECT topic_id, topic_label, first_seen_date, last_seen_date,
                   peak_date, total_mentions
            FROM topic_lifetime
            ORDER BY first_seen_date DESC
            LIMIT 500
        """
        df = load_df(query)

        if df.empty:
            st.info("Aucune donnÃ©e de lifetime pour les topics.")
            return

        # ğŸ”§ Normaliser les colonnes de dates au format datetime
        df["first_seen_date"] = pd.to_datetime(df["first_seen_date"])
        df["last_seen_date"] = pd.to_datetime(df["last_seen_date"])
        df["peak_date"] = pd.to_datetime(df["peak_date"])

        st.dataframe(df)

        st.markdown("**Topics les plus persistants :**")
        longest = (
            df.assign(
                duration_days=(df["last_seen_date"] - df["first_seen_date"]).dt.days + 1
            )
            .sort_values("duration_days", ascending=False)
            .head(20)
        )
        st.dataframe(longest)

        st.markdown("**Topics les plus couverts (mentions totales) :**")
        most_covered = df.sort_values("total_mentions", ascending=False).head(20)
        st.dataframe(most_covered)


    elif analysis == "Theme Lifetime":
        st.subheader("ğŸ­ Theme Lifetime")

        query = """
            SELECT theme, start_date, end_date, peak_date, total_mentions
            FROM theme_lifetime
            ORDER BY start_date DESC
            LIMIT 500
        """
        df = load_df(query)

        if df.empty:
            st.info("Aucune donnÃ©e de lifetime pour les thÃ¨mes.")
            return

        # ğŸ”§ Normaliser les colonnes de dates
        df["start_date"] = pd.to_datetime(df["start_date"])
        df["end_date"] = pd.to_datetime(df["end_date"])
        df["peak_date"] = pd.to_datetime(df["peak_date"])

        st.dataframe(df)

        st.markdown("**ThÃ¨mes les plus persistants :**")
        df = df.assign(
            duration_days=(df["end_date"] - df["start_date"]).dt.days + 1
        )
        longest = df.sort_values("duration_days", ascending=False).head(20)
        st.dataframe(longest)

        st.markdown("**ThÃ¨mes les plus couverts (mentions totales) :**")
        most_covered = df.sort_values("total_mentions", ascending=False).head(20)
        st.dataframe(most_covered)
